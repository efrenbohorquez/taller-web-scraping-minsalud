"""
Script de prueba para el scraper - VersiÃ³n optimizada y rÃ¡pida
"""
import sys
import os
from pathlib import Path

# Configurar codificaciÃ³n UTF-8 para Windows
if sys.platform == 'win32':
    os.system('chcp 65001 >nul 2>&1')
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')

sys.path.insert(0, str(Path(__file__).parent / "src"))

from scraper import MinSaludScraper
import requests
from bs4 import BeautifulSoup

def test_conexion():
    """Prueba de conexiÃ³n bÃ¡sica"""
    print("="*60)
    print("ğŸ§ª TEST 1: ConexiÃ³n al sitio web")
    print("="*60)
    
    urls_prueba = [
        "https://www.minsalud.gov.co",
        "https://www.minsalud.gov.co/Normatividad/Paginas/normatividad.aspx",
        "https://www.minsalud.gov.co/Normatividad_Nuevo/Paginas/Normatividad.aspx"
    ]
    
    for url in urls_prueba:
        try:
            response = requests.get(url, timeout=10)
            print(f"âœ… {url}")
            print(f"   CÃ³digo: {response.status_code}")
            if response.status_code == 200:
                print(f"   âœ… URL VÃLIDA ENCONTRADA!")
                return url
        except Exception as e:
            print(f"âŒ {url} - Error: {str(e)[:50]}")
    
    return None

def test_extraccion_basica(url):
    """Prueba extracciÃ³n de links bÃ¡sica"""
    print("\n" + "="*60)
    print("ğŸ§ª TEST 2: ExtracciÃ³n de hipervÃ­nculos")
    print("="*60)
    
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, 'lxml')
        
        # Buscar divs con diferentes clases posibles
        divs_posibles = [
            'container_blanco',
            'container',
            'content',
            'main-content'
        ]
        
        container_div = None
        for div_class in divs_posibles:
            container_div = soup.find('div', class_=div_class)
            if container_div:
                print(f"âœ… Contenedor encontrado: '{div_class}'")
                break
        
        if not container_div:
            print("âš ï¸  No se encontrÃ³ contenedor especÃ­fico, usando body completo")
            container_div = soup.find('body')
        
        # Contar links
        links = container_div.find_all('a') if container_div else []
        links_aspx = [l for l in links if l.get('href', '').endswith('.aspx')]
        links_pdf = [l for l in links if l.get('href', '').endswith('.pdf')]
        
        print(f"ğŸ“Š Total de links: {len(links)}")
        print(f"   - Links .aspx: {len(links_aspx)}")
        print(f"   - Links .pdf: {len(links_pdf)}")
        
        if links_pdf:
            print(f"\nğŸ“„ Ejemplos de PDFs encontrados:")
            for pdf_link in links_pdf[:3]:
                print(f"   - {pdf_link.get('href')}")
        
        return len(links) > 0
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

def test_scraper_completo():
    """Test del scraper completo con lÃ­mites"""
    print("\n" + "="*60)
    print("ğŸ§ª TEST 3: Scraper completo (modo prueba)")
    print("="*60)
    
    scraper = MinSaludScraper()
    
    # Modificar lÃ­mites para prueba rÃ¡pida
    print("âš¡ ConfiguraciÃ³n de prueba rÃ¡pida:")
    print("   - MÃ¡ximo 5 pÃ¡ginas a visitar")
    print("   - MÃ¡ximo 3 PDFs a descargar")
    print("   - Tiempo de espera reducido")
    
    # Ejecutar solo crawling limitado
    print("\nğŸ•·ï¸  Iniciando crawling limitado...")
    try:
        scraper.crawl_sitio_web(max_paginas=5)
        print("âœ… Crawling completado")
        return True
    except Exception as e:
        print(f"âŒ Error en crawling: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("\n" + "ğŸš€"*30)
    print(" "*20 + "SUITE DE PRUEBAS - MINSALUD SCRAPER")
    print("ğŸš€"*30 + "\n")
    
    # Test 1: ConexiÃ³n
    url_valida = test_conexion()
    if not url_valida:
        print("\nâŒ No se pudo conectar a ninguna URL vÃ¡lida")
        print("ğŸ’¡ Verifica tu conexiÃ³n a internet o que el sitio estÃ© disponible")
        return
    
    # Test 2: ExtracciÃ³n bÃ¡sica
    if not test_extraccion_basica(url_valida):
        print("\nâš ï¸  La extracciÃ³n bÃ¡sica fallÃ³, pero continuamos...")
    
    # Test 3: Scraper completo
    test_scraper_completo()
    
    print("\n" + "="*60)
    print("âœ… SUITE DE PRUEBAS COMPLETADA")
    print("="*60)
    print("\nğŸ’¡ Para ejecutar el scraper completo sin lÃ­mites:")
    print("   python main.py")
    print("\nğŸ’¡ Para ver opciones:")
    print("   python main.py --help")

if __name__ == "__main__":
    main()
